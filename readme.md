```md
# OllamaFit  

A simple command-line tool to check which **Ollama AI models** your system can run smoothly.  

## ğŸš€ What is OllamaFit?  

OllamaFit scans your system's **CPU, RAM, and GPU** to see which Ollama models will work well. It fetches the latest model info from Ollamaâ€™s website, calculates memory needs based on model size & quantization, and gives you a clear **compatibility report**.  

## ğŸ”¥ Features  

âœ… **Automatic hardware detection** (CPU, RAM, GPU/VRAM)  
âœ… **Search models** by name (e.g., `llama`, `code`, etc.)  
âœ… **Smart memory calculations** based on model size & quantization  
âœ… **Supports CPU-only inference** for small quantized models  
âœ… **Handles Apple Silicon shared memory**  
âœ… **Caches results** to reduce web requests  

## ğŸ›  Installation  

```bash
# Clone the repo
git clone https://github.com/yourusername/ollamafit.git
cd ollamafit

# Create a virtual environment
python -m venv ollamafit
source ollamafit/bin/activate  # Windows: ollamafit\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

## â–¶ï¸ How to Use  

Check all compatible models:  
```bash
python ollamafit_cli.py
```

Search for specific models:  
```bash
python ollamafit_cli.py --search llama  
```

Limit results (for quick testing):  
```bash
python ollamafit_cli.py --max-models 5
```

Save results to a file:  
```bash
python ollamafit_cli.py --output my_results.json
```

## âš¡ How It Works  

1. Detects your systemâ€™s **CPU, RAM, and GPU** specs  
2. Fetches the latest **Ollama models**  
3. **Calculates memory needs** for each model based on:  
   - Model **size (parameters)**  
   - **Quantization** (bit-level optimization)  
   - **Extra overhead** for runtime needs  
4. Applies **special handling** for:  
   - **Apple Silicon (shared RAM/VRAM)**  
   - **CPU-only inference for small models**  
5. **Shows which models you can run**  

## âœ… Whatâ€™s Working Well  

- **Accurate Hardware Checks**: RAM, VRAM, CPU & GPU detection  
- **Quantization Support**: Accounts for memory savings from model quantization  
- **Reasonable Memory Overhead**: +20% VRAM, +30% RAM for extra usage  
- **CPU-Only Compatibility**: Allows small models (â‰¤3B on **4GB RAM**, â‰¤7B on **6GB RAM**)  

## âŒ What Can Be Improved  

ğŸ”¹ **Better VRAM Estimation** â†’ Current formula is close, but doesnâ€™t factor in **context length** & **batch size**  
ğŸ”¹ **More Detailed Quantization Handling** â†’ Different methods (QLoRA, GPTQ, GGUF) impact memory differently  
ğŸ”¹ **Context Length Awareness** â†’ Long prompts use more memory, but we donâ€™t account for this yet
ğŸ”¹ **Better Integrated GPU Handling** â†’ Some GPUs (especially on Windows/Linux) can use shared RAM  

## ğŸ“œ Requirements  

- **Python 3.7+**  
- **Internet connection** (to fetch model data)  
- **Dependencies**: `requests`, `beautifulsoup4`, `psutil`  

## ğŸ“ License  

**MIT License** â€“ Free to use, modify & improve!  

---

ğŸ”¹ **Feedback & Contributions Welcome!** ğŸš€